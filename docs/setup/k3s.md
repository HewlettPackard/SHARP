# Setup k3s (on HPE)

This assumes a clean-ish environment, but it helps that Docker is setup and installed.

## Installing the cluster

The first part is installing and running the cluster.
If the cluster already runs and you just need to access it, skip to the next section!

1. Preparing the DNS environment:

    Currently there is an [issue with using `k3s` on our Ubuntu machines](https://github.hpe.com/lianjie-cao/kube-install-hpe/issues/2).
    Our default `/etc/resolv.conf` looks like this:

    ```conf
    # Generated by NetworkManager
    search labs.hpecorp.net
    nameserver 10.93.244.180
    nameserver 127.0.0.53
    ```

    Adapt it to comment out the last line so that `k3s` does not silently replace our DNS configuration with Google's DNS:

    ```conf
    # Generated by NetworkManager
    search labs.hpecorp.net
    nameserver 10.93.244.180
    # nameserver 127.0.0.53
    ```

1. Installing `k3s` using Lianjie's helper script:

    ```sh
    # clone the kube-install-hpe repository
    git clone git@github.hpe.com:lianjie-cao/kube-install-hpe.git
    cd kube-install-hpe

    # install k3s with the containerd backend (docker is also available with --cri docker but harder to use and might be deprecated)
    ./deploy-k3s.sh -o install --server
    ```

1. Getting access to the cluster:

    For a local user, we must first get access to the cluster configuration.
    The easiest way to achieve that is to share the cluster config with the team:

    ```sh
    sudo chown -hR $USER:docker /etc/rancher
    ```

    We add the `docker` group to give everyone who has access to Docker access to the cluster as well (in theory -- this is untested).

1. Add the container registry mirror:

    We do not have proper access to Docker Hub from HPE clusters at the moment, hence we need to add the container registry mirror to pull containers.
    This is also required to run a few of the system pods k3s needs.
    Install the necessary certificate:

    ```sh
    # download the HPE private CA
    wget http://16.143.20.46:8080/rock/jenkins/hpe_root_ca/HP_Ent_Private_SSL_CA.pem

    # move it to our local certificate store
    # need to convert it to .crt, otherwise Ubuntu will not pick it up
    sudo mv HP_Ent_Private_SSL_CA.pem /usr/local/share/ca-certificates/HP_Ent_Private_SSL_CA.crt
    sudo update-ca-certificates --fresh
    ```

    Then simply add a file `/etc/rancher/k3s/registries.yaml`:

    ```yml
    mirrors:
      docker.io:
        endpoint:
          - "bd-harbor-registry.mip.storage.hpecorp.net"
        rewrite:
          "^(.*)$": "dockerhub/$1"
    ```

    Substitute your preferred container endpoint.
    Restart `k3s` with `sudo systemctl restart k3s`.

## Accessing the cluster

If the cluster is installed, you just need to tell your systems to use it.
By default you can use the `kubectl` and other utilities bundled with `k3s` by executing `k3s kubectl`.
If you want your installed `kubectl` to use the `k3s` cluster (this also applies to other tools that talk to the cluster directly, such as Fission), adapt your `KUBECONFIG` environment variable:

```sh
echo "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml" >> ~/.bashrc
source ~/.bashrc
```

## Debugging

You can run a container on `k3s` and then access the shell on that container:

```sh
kubectl run busybox --restart=Never --image=busybox:1.28 -- sleep 3600
kubectl exec busybox -- nslookup www.google.com
```

## Exposing Functions to the Outside World

This is a work in progress as Kubernetes networking appears to be hard.
Kubernetes-specific instructions are only tested for `k3s`.

To expose Fission functions to the outside world (outside world in this case referring to anything outside our cluster), we can use HTTP triggers.
These work similarly to `fission fn test` but can be called from anywhere that has network access to the Fission Router.
This last part is crucial, as none of our containers are by default accessible from outside the cluster.

In Kubernetes, accessing resources from the outside requires something called an "Ingress".
As an alternative way, a workaround is to temporarily port forward the Fission router.
This is described below.

1. Create an HTTP Trigger for your function:

    Assuming you have a function `matmult-mt` installed on Fission, use this command to make it accessible at the route `/matmul`:

    ```sh
    fission httptrigger create --url /matmul --method POST --function matmult-mt 
    ```

    This will create a trigger for the `POST` method, but you can also leave this out.
    Our function reads from the request body, so POST only makes sense here.

### Using an Ingress Controller

To use the ingress controller, we simply have to tell Traefik about our Fission router.

1. Find out about our services:

    First, let's check that our Router is up and running:

    ```sh
    $ kubectl get svc -n fission
    NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
    controller   NodePort    10.43.54.138   <none>        80:31313/TCP   52m
    executor     ClusterIP   10.43.50.108   <none>        80/TCP         52m
    storagesvc   ClusterIP   10.43.128.99   <none>        80/TCP         52m
    router       NodePort    10.43.12.52    <none>        80:31314/TCP   52m
    ```

    We see that our `router` service is up and running and has port 80 available.
    This appears to be mapped to 31314 on our host already, but we want a better, fixed port.

1. Create an Ingress Deployment:

    Create a new file `k3s-route-fission.yml` that will hold our Ingress deployment:

    ```yml
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: fission-router
      annotations:
        ingress.kubernetes.io/ssl-redirect: "false"
    spec:
      rules:
      - http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: router
              port:
                number: 80
    ```

1. Deploy the ingress with `kubectl create -f k3s-route-fission.yml -n fission`.

    You can check that everything is correct:

    ```sh
    $ kubectl get ingress -n fission
    NAME             CLASS    HOSTS   ADDRESS        PORTS   AGE
    fission-router   <none>   *       10.93.232.57   80      9m37s
    ```

1. Call the function:

    Finally, we can call the function:

    ```sh
    curl -X "1" "http://localhost:80/matmul"
    ```

If you want to pass data to your function, you can use HTTP headers and the HTTP body (probably `POST` only).
In binary environments, HTTP headers will be available as environment variables starting with `HTTP_`, while the body will be passed to `STDIN`.

### Using temporary port forwarding

The Router is the component of Fission that is responsible for accepting HTTP requests and invoking functions.
By default, that Router uses port `8888` to expose its own HTTP server, but that is local to that container.
So we need to bind the container port `8888` to our host somehow.
This, in theory, is what an ingress controller is for, but for now that is too complicated so we can use `kubectl port-forward` for temporary port forwarding:

```sh
kubectl port-forward -n fission $(kubectl get pods -n fission -l application=fission-router --output=name) 8888:8888
```

That will forward our localhost port `8888` to the router for the duration that command has not exited.

## Using the HPE MIP Registry

This depends on which container backend you use.
Setting up the first step is the same, installing certificates:

```sh
# download the HPE private CA
wget http://16.143.20.46:8080/rock/jenkins/hpe_root_ca/HP_Ent_Private_SSL_CA.pem

# move it to our local certificate store
# need to convert it to .crt, otherwise Ubuntu will not pick it up
sudo mv HP_Ent_Private_SSL_CA.pem /usr/local/share/ca-certificates/HP_Ent_Private_SSL_CA.crt
sudo update-ca-certificates --fresh
```

### containerd

**For `containerd`**, configure the registry for `k3s` by creating `/etc/rancher/k3s/registries.yaml`:

```yml
mirrors:
  docker.io:
    endpoint:
      - "bd-harbor-registry.mip.storage.hpecorp.net"
    rewrite:
      "^(.*)$": "dockerhub/$1"
configs:
  "bd-harbor-registry.mip.storage.hpecorp.net":
    auth:
      username: robot$hsa+etc1-k3s
      password: <<YOUR AUTHENTICATION KEY>>
```

Then restart `k3s` with `sudo systemctl restart k3s`

We have seen errors where the `rewrite` does not work properly.
For example, when pulling the image `fission/fetcher:v1.16.0`, we expect `containerd` to pull `bd-harbor-registry.mip.storage.hpecorp.net/dockerhub/fission/fetcher:v1.16.0` but it would leave out the `dockerhub` path and run into errors.
As a workaround, you can manually pull an image from the pull-through cache and re-tag it:

```sh
# use the containerd CLI interface
sudo /usr/local/bin/k3s ctr image pull bd-harbor-registry.mip.storage.hpecorp.net/dockerhub/fission/fetcher:v1.16.0

# we now have the image on our machine but the name is not as required by containerd to find that image
# we can re-tag it to help containerd find it when it gets a request
sudo /usr/local/bin/k3s ctr image tag bd-harbor-registry.mip.storage.hpecorp.net/dockerhub/fission/fetcher:v1.16.0 fission/fetcher:v1.16.0
```

### Docker

**For Docker**, restart your Docker daemon to pick up the new certificates:

```sh
sudo systemctl restart docker
```

The login to Docker with `docker login` and your credentials.
Make sure to specify the registry, e.g.:

```sh
$ docker login bd-harbor-registry.mip.storage.hpecorp.net
Username: robot$hsa+etc1-k3s
Password: ...
```

This does not work properly at the moment!

## Accessing k3s from outside the cluster

It might be helpful to access `k3s` from a machine that is not part of the cluster.
Locally installed tools such as `fission-cli` or `kubectl` can use the cluster if you copy the `k3s` configuration file and replace the `server` value.
Of course, network access to the cluster (default port is `6443`) should be possible.

```sh
# replace your values accordingly
username=USER
server=SERVER
scp $username@$server:/etc/rancher/k3s/k3s.yaml .

sed -i "s/127.0.0.1/$server/g" k3s.yaml

mv k3s.yaml ~/.kube/config
```

Both `kubectl` and `fission-cli` will use this file as a default when looking to access Kubernetes.
If you want, you can also keep this file elsewhere and just edit the `KUBECONFIG` environment variable.
